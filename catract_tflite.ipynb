{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2001abdulrehman/FYP/blob/main2/catract_tflite.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vkhz-jXCJqJE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.lite.python import interpreter as interpreter_wrapper\n",
        "from PIL import Image\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOygo5bxDAMH",
        "outputId": "3cd32701-53a4-4e22-d717-dafffd71c2c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 1180 images belonging to 2 classes.\n",
            "Epoch 1/20\n",
            "37/37 [==============================] - 101s 3s/step - loss: 0.7856 - accuracy: 0.6593\n",
            "Epoch 2/20\n",
            "37/37 [==============================] - 100s 3s/step - loss: 0.4224 - accuracy: 0.8195\n",
            "Epoch 3/20\n",
            "37/37 [==============================] - 99s 3s/step - loss: 0.3797 - accuracy: 0.8347\n",
            "Epoch 4/20\n",
            "37/37 [==============================] - 104s 3s/step - loss: 0.4196 - accuracy: 0.8110\n",
            "Epoch 5/20\n",
            "37/37 [==============================] - 101s 3s/step - loss: 0.3515 - accuracy: 0.8424\n",
            "Epoch 6/20\n",
            "37/37 [==============================] - 98s 3s/step - loss: 0.2975 - accuracy: 0.8720\n",
            "Epoch 7/20\n",
            "37/37 [==============================] - 99s 3s/step - loss: 0.2927 - accuracy: 0.8822\n",
            "Epoch 8/20\n",
            "37/37 [==============================] - 100s 3s/step - loss: 0.2798 - accuracy: 0.8797\n",
            "Epoch 9/20\n",
            "37/37 [==============================] - 105s 3s/step - loss: 0.2582 - accuracy: 0.9051\n",
            "Epoch 10/20\n",
            "37/37 [==============================] - 102s 3s/step - loss: 0.2513 - accuracy: 0.9000\n",
            "Epoch 11/20\n",
            "37/37 [==============================] - 100s 3s/step - loss: 0.2446 - accuracy: 0.9025\n",
            "Epoch 12/20\n",
            "37/37 [==============================] - 97s 3s/step - loss: 0.2583 - accuracy: 0.8932\n",
            "Epoch 13/20\n",
            "37/37 [==============================] - 99s 3s/step - loss: 0.1816 - accuracy: 0.9297\n",
            "Epoch 14/20\n",
            "37/37 [==============================] - 105s 3s/step - loss: 0.1731 - accuracy: 0.9322\n",
            "Epoch 15/20\n",
            "37/37 [==============================] - 100s 3s/step - loss: 0.2326 - accuracy: 0.9076\n",
            "Epoch 16/20\n",
            "37/37 [==============================] - 100s 3s/step - loss: 0.1609 - accuracy: 0.9390\n",
            "Epoch 17/20\n",
            "37/37 [==============================] - 97s 3s/step - loss: 0.1550 - accuracy: 0.9492\n",
            "Epoch 18/20\n",
            "37/37 [==============================] - 99s 3s/step - loss: 0.1486 - accuracy: 0.9542\n",
            "Epoch 19/20\n",
            "37/37 [==============================] - 98s 3s/step - loss: 0.1326 - accuracy: 0.9542\n",
            "Epoch 20/20\n",
            "37/37 [==============================] - 99s 3s/step - loss: 0.1170 - accuracy: 0.9576\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "# Extract the data from the zip file\n",
        "with zipfile.ZipFile('/content/drive/MyDrive/New Model dataset/train.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('dataset-tflite')\n",
        "\n",
        "# Define paths to the training data\n",
        "train_dir = 'dataset-tflite'\n",
        "\n",
        "# Create an image data generator\n",
        "train_data_generator = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "train_generator = train_data_generator.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "# Build the CNN model\n",
        "model = keras.Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3),\n",
        "           data_format=\"channels_last\"),  # Input layer with Conv2D\n",
        "    MaxPooling2D(2, 2),  # Max-pooling layer\n",
        "    Conv2D(64, (3, 3), activation='relu'),  # Second Conv2D layer\n",
        "    MaxPooling2D(2, 2),  # Second Max-pooling layer\n",
        "    Conv2D(128, (3, 3), activation='relu'),  # Third Conv2D layer\n",
        "    MaxPooling2D(2, 2),  # Third Max-pooling layer\n",
        "    Flatten(),  # Flatten layer\n",
        "    Dense(512, activation='relu'),  # Dense (fully connected) layer\n",
        "    Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=len(train_generator),\n",
        "    epochs=20,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Save the trained model\n",
        "model.save('cataract_detection_model.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**QUANTIZE AND CONVERT TO TFLITE**"
      ],
      "metadata": {
        "id": "usmUmtrDe3Dz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fpsziT9Jzj1",
        "outputId": "33b1d881-438a-4df6-e4b8-4e72b3d5aa28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model converted and saved to your_model.tflite\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Save the trained model to an h5 file\n",
        "model.save('/content/cataract_detection_model.h5')\n",
        "\n",
        "# Load the trained model\n",
        "loaded_model = tf.keras.models.load_model('/content/cataract_detection_model.h5')\n",
        "\n",
        "# Convert the model to a TFLite quantized model\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(loaded_model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]  # Apply optimizations including quantization\n",
        "quantized_tflite_model = converter.convert()\n",
        "\n",
        "# Save the quantized TFLite model to a file\n",
        "with open('/content/quantized_model.tflite', 'wb') as f:\n",
        "    f.write(quantized_tflite_model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUOPhDkT_1UY"
      },
      "source": [
        "**TESTING**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with zipfile.ZipFile('/content/drive/MyDrive/Catracts/Test.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('test_dataset')"
      ],
      "metadata": {
        "id": "QPYcDn2Lfg0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qiLT__Yv_z9F",
        "outputId": "70e9c5c0-08cd-49ec-8288-4a9998555eb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 212 images belonging to 2 classes.\n",
            "Accuracy on the test set: 79.72%\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Define the path to the extracted test dataset directory\n",
        "test_dir = '/content/test_dataset'\n",
        "#/content/drive/MyDrive/Catracts/your_model.tflite\n",
        "# Load the TensorFlow Lite model\n",
        "tflite_model_path = '/content/drive/MyDrive/Catracts/quantized_model.tflite'\n",
        "interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "# Create an image data generator for test data\n",
        "test_data_generator = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "test_generator = test_data_generator.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=1,  # Set batch_size to 1 to handle images one by one\n",
        "    class_mode='binary',\n",
        "    shuffle=False  # Ensure no shuffling for evaluation\n",
        ")\n",
        "\n",
        "# Initialize variables to store true labels and predicted labels\n",
        "true_labels = []\n",
        "predicted_labels = []\n",
        "\n",
        "# Iterate through the test set to get predictions\n",
        "for i in range(len(test_generator)):\n",
        "    image, label = test_generator[i]\n",
        "\n",
        "    # Preprocess the image to match the input shape expected by the model\n",
        "    image = img_to_array(image[0])  # Convert from PIL image to numpy array\n",
        "    image = np.expand_dims(image, axis=0)  # Add a batch dimension\n",
        "\n",
        "    interpreter.set_tensor(input_details[0]['index'], image)\n",
        "    interpreter.invoke()\n",
        "    tflite_predictions = interpreter.get_tensor(output_details[0]['index'])\n",
        "    predicted_labels.append(tflite_predictions[0][0])\n",
        "    true_labels.append(label[0])\n",
        "\n",
        "# Convert the lists to numpy arrays for comparison\n",
        "true_labels = np.array(true_labels)\n",
        "predicted_labels = np.array(predicted_labels)\n",
        "\n",
        "# Calculate accuracy\n",
        "threshold = 0.5  # Threshold for binary classification\n",
        "predicted_classes = (predicted_labels >= threshold).astype('int32')\n",
        "correct_predictions = np.sum(predicted_classes == true_labels)\n",
        "accuracy = correct_predictions / len(true_labels)\n",
        "\n",
        "print(f\"Accuracy on the test set: {accuracy * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guxyNKmkBcWY"
      },
      "source": [
        "**RANDOM TEST**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load the TFLite model\n",
        "tflite_path = '/content/drive/MyDrive/Catracts/quantized_model.tflite'\n",
        "interpreter = interpreter_wrapper.Interpreter(model_path=tflite_path)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Define the path to the folder containing mixed cataract and normal images\n",
        "image_folder = '/content/drive/MyDrive/Catracts/RANDOM'\n",
        "\n",
        "# Create empty lists to store cataract and normal image filenames\n",
        "cataract_images = []\n",
        "normal_images = []\n",
        "\n",
        "# Iterate over the images in the folder and classify them\n",
        "for filename in os.listdir(image_folder):\n",
        "    if filename.endswith('.jpg') or filename.endswith('.png'):\n",
        "        image_path = os.path.join(image_folder, filename)\n",
        "\n",
        "        # Load the image\n",
        "        image = Image.open(image_path)\n",
        "        image = image.resize((150, 150))  # Resize to match the model's input size\n",
        "        image = tf.convert_to_tensor(image, dtype=tf.float32) / 255.0  # Normalize the image\n",
        "        image = image.numpy()\n",
        "        image = image[np.newaxis, ...]  # Add a batch dimension\n",
        "\n",
        "        # Set input tensor\n",
        "        input_details = interpreter.get_input_details()\n",
        "        interpreter.set_tensor(input_details[0]['index'], image)\n",
        "\n",
        "        # Invoke the interpreter\n",
        "        interpreter.invoke()\n",
        "\n",
        "        # Get the output tensor\n",
        "        output_details = interpreter.get_output_details()\n",
        "        output = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "        # Classify the image\n",
        "        is_cataract = output[0][0] < 0.5  # Adjust the threshold as needed\n",
        "\n",
        "        # Separate cataract and normal images\n",
        "        if is_cataract:\n",
        "            cataract_images.append(filename)\n",
        "        else:\n",
        "            normal_images.append(filename)\n",
        "\n",
        "# Display the cataract and normal image filenames\n",
        "print(\"Cataract Images:\")\n",
        "for filename in cataract_images:\n",
        "    print(filename)\n",
        "\n",
        "print(\"\\nNormal Images:\")\n",
        "for filename in normal_images:\n",
        "    print(filename)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44AklCO0YsEc",
        "outputId": "d0d1edaa-7d0c-4f42-fa34-fad611e27ace"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cataract Images:\n",
            "B (3).png\n",
            "B (2).png\n",
            "B (4).png\n",
            "B (1).png\n",
            "B (5).png\n",
            "\n",
            "Normal Images:\n",
            "A (4).png\n",
            "A (3).png\n",
            "A (2).png\n",
            "A (1).png\n",
            "A (5).png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Single Image Test**"
      ],
      "metadata": {
        "id": "F5qbR93igGS6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def classify_single_image(image_path, tflite_model_path):\n",
        "    # Load the TFLite model\n",
        "    interpreter = interpreter_wrapper.Interpreter(model_path=tflite_model_path)\n",
        "    interpreter.allocate_tensors()\n",
        "\n",
        "    # Load the image\n",
        "    image = Image.open(image_path)\n",
        "    image = image.resize((150, 150))  # Resize to match the model's input size\n",
        "    image = np.array(image, dtype=np.float32) / 255.0  # Normalize the image\n",
        "    image = np.expand_dims(image, axis=0)  # Add a batch dimension\n",
        "\n",
        "    # Set input tensor\n",
        "    input_details = interpreter.get_input_details()\n",
        "    interpreter.set_tensor(input_details[0]['index'], image)\n",
        "\n",
        "    # Invoke the interpreter\n",
        "    interpreter.invoke()\n",
        "\n",
        "    # Get the output tensor\n",
        "    output_details = interpreter.get_output_details()\n",
        "    output = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "    # Classify the image\n",
        "    is_cataract = output[0][0] < 0.5  # Adjust the threshold as needed\n",
        "\n",
        "    # Return classification result\n",
        "    return \"Cataract\" if is_cataract else \"Normal\"\n",
        "\n",
        "#  path to your single image\n",
        "image_to_classify = '/content/f0229fdf-f0fd-4c8b-a5c2-7eb4fa4f4681.jpg'\n",
        "\n",
        "#  path to your TFLite model\n",
        "model_path = '/content/drive/MyDrive/Catracts/quantized_model.tflite'\n",
        "\n",
        "result = classify_single_image(image_to_classify, model_path)\n",
        "print(f\"The image is classified as: {result}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZZsZemFB8gO",
        "outputId": "c9d49777-7b09-4de2-9e69-141a94d2cc31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The image is classified as: Normal\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Rrj4AqP6KStpudv-WHpqJXgyiZVw3yqq",
      "authorship_tag": "ABX9TyNqZVvFa1PdyRmtvVxSZHZa",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}